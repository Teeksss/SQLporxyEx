# Fluentd Configuration for Enterprise SQL Proxy
# Created: 2025-05-29 12:29:34 UTC by Teeksss
# Version: 2.0.0

<source>
  @type forward
  port 24224
  bind 0.0.0.0
</source>

# Backend Application Logs
<source>
  @type tail
  path /var/log/backend/*.log
  pos_file /var/log/fluentd/backend.log.pos
  tag sql_proxy.backend
  format json
  time_format %Y-%m-%d %H:%M:%S
  time_key timestamp
  refresh_interval 10
</source>

# Nginx Access Logs
<source>
  @type tail
  path /var/log/nginx/access.log
  pos_file /var/log/fluentd/nginx_access.log.pos
  tag sql_proxy.nginx.access
  format nginx
  refresh_interval 10
</source>

# Nginx Error Logs
<source>
  @type tail
  path /var/log/nginx/error.log
  pos_file /var/log/fluentd/nginx_error.log.pos
  tag sql_proxy.nginx.error
  format /^(?<time>\d{4}/\d{2}/\d{2} \d{2}:\d{2}:\d{2}) \[(?<log_level>\w+)\] (?<pid>\d+).(?<tid>\d+): (?<message>.*)$/
  time_format %Y/%m/%d %H:%M:%S
  refresh_interval 10
</source>

# PostgreSQL Logs
<source>
  @type tail
  path /var/log/postgresql/*.log
  pos_file /var/log/fluentd/postgresql.log.pos
  tag sql_proxy.postgresql
  format /^(?<time>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}.\d{3} \w+) \[(?<pid>\d+)\] (?<level>\w+):  (?<message>.*)$/
  time_format %Y-%m-%d %H:%M:%S.%L %Z
  refresh_interval 10
</source>

# Filter to parse JSON logs
<filter sql_proxy.backend>
  @type parser
  key_name message
  reserve_data true
  <parse>
    @type json
  </parse>
</filter>

# Add hostname and environment
<filter sql_proxy.**>
  @type record_transformer
  <record>
    hostname "#{Socket.gethostname}"
    environment "#{ENV['NODE_ENV'] || 'production'}"
    service_name sql_proxy
    log_source fluentd
  </record>
</filter>

# Error detection and alerting
<filter sql_proxy.**>
  @type grep
  <regexp>
    key level
    pattern ^(ERROR|CRITICAL|FATAL)$
  </regexp>
  <regexp>
    key log_level
    pattern ^(error|crit|alert|emerg)$
  </regexp>
</filter>

# Security event detection
<filter sql_proxy.**>
  @type grep
  <regexp>
    key message
    pattern (authentication|authorization|security|intrusion|attack)
  </regexp>
</filter>

# Output to Elasticsearch
<match sql_proxy.**>
  @type elasticsearch
  host elasticsearch
  port 9200
  logstash_format true
  logstash_prefix sql-proxy
  logstash_dateformat %Y.%m.%d
  include_tag_key true
  tag_key @log_name
  flush_interval 10s
  
  # Buffer configuration
  <buffer>
    @type file
    path /var/log/fluentd/buffer
    flush_mode interval
    flush_interval 10s
    chunk_limit_size 10MB
    queue_limit_length 32
    retry_forever false
    retry_max_times 3
  </buffer>
</match>

# Output errors to file as backup
<match sql_proxy.error.**>
  @type file
  path /var/log/fluentd/errors
  append true
  time_slice_format %Y%m%d
  time_slice_wait 10m
  time_format %Y-%m-%dT%H:%M:%S%z
  compress gzip
  
  <format>
    @type json
  </format>
</match>

# System metrics collection
<source>
  @type systemd
  tag systemd
  path /var/log/journal
  filters [{ "_SYSTEMD_UNIT": "docker.service" }]
  read_from_head true
  strip_underscores true
</source>

# Forward critical logs to external systems
<match sql_proxy.critical.**>
  @type forward
  require_ack_response true
  ack_response_timeout 30
  
  <server>
    name external-log-server
    host "#{ENV['EXTERNAL_LOG_HOST']}"
    port "#{ENV['EXTERNAL_LOG_PORT'] || 24224}"
  </server>
  
  <buffer>
    @type file
    path /var/log/fluentd/external_buffer
    flush_mode interval
    flush_interval 5s
    chunk_limit_size 1MB
    queue_limit_length 16
  </buffer>
</match>